---
title: "aws sap 문제 풀이 3주차 문제풀이 3번"
collection: memo
type: "aws"
permalink: /memo/aws-sap-week3-3
date: 2023-03-12
venue: "sap"
---


# 문제

질문 28
A company processes several petabytes of images submitted by users on their photo hosting site every month. Each month, the images are processed in its on-premises data center by a High-Performance Computing (HPC) cluster with a capacity of 5,000 cores and 10 petabytes of data. Processing a month’s worth of images by thousands of jobs running in parallel takes about a week and the processed images are stored on a network file server, which also backups the data to a disaster recovery site.
The current data center is nearing its capacity so the users are forced to spread the jobs within the course of the month. This is not ideal for the requirement of the jobs, so the Solutions Architect was tasked to design a scalable solution that can exceed the current capacity with the least amount of management overhead while maintaining the current level of durability.
Which of the following solutions will meet the company's requirements while being cost-effective?


회사는 매달 사진 호스팅 사이트에 사용자가 제출한 페타바이트의 이미지를 처리합니다. 매달 이미지는 5,000개의 코어와 10페타바이트의 데이터 용량을 갖춘 HPC(고성능 컴퓨팅) 클러스터에 의해 온프레미스 데이터 센터에서 처리됩니다. 병렬로 실행되는 수천 개의 작업(job)으로 한 달 분량의 이미지를 처리하는 데 약 일주일이 걸리며 처리된 이미지는 네트워크 파일 서버에 저장되며 데이터는 disaster recovery site에도 백업됩니다.
현재 데이터 센터의 용량이 거의 다 차서 사용자는 한 달 안에 작업(job)을 분산해야 합니다. 이는 작업 요구 사항에 적합하지 않기 때문에 Solutions Architect는 현재 수준의 내구성(durability)을 유지하면서 최소한의 관리 overhead로 현재 용량을 초과할 수 있는 확장 가능한(scalable) 솔루션을 설계해야 합니다.
다음 중 비용 효율적이면서 회사의 요구 사항을 충족하는 솔루션은 무엇입니까?


![](/assets/2023-03-12-19-20-25.png)

1. Using a combination of On-demand and Reserved Instances as Task Nodes, create an EMR cluster that will use Spark to pull the raw data from an Amazon S3 bucket. List the jobs that need to be processed by the EMR cluster on a DynamoDB table. Store the processed images on a separate Amazon S3 bucket.
2. Utilize AWS Batch with Managed Compute Environments to create a fleet using Spot Instances. Store the raw data on an Amazon S3 bucket. Create jobs on AWS Batch Job Queues that will pull objects from the Amazon S3 bucket and temporarily store them to the EC2 EBS volumes for processing. Send the processed images back to another Amazon S3 bucket.
3. Package the executable file for the job in a Docker image stored on Amazon Elastic Container Registry (Amazon ECR). Run the Docker images on Amazon Elastic Kubernetes Service (Amazon EKS). Auto Scaling can be handled automatically by EKS. Store the raw data temporarily on Amazon EBS SC1 volumes and then send the images to an Amazon S3 bucket after processing.
4. Create an Amazon SQS queue and submit the list of jobs to be processed. Create an Auto Scaling Group of Amazon EC2 Spot Instances that will process the jobs from the SQS queue. Share the raw data across all the instances using Amazon EFS. Store the processed images in an Amazon S3 bucket for long term storage.


# 풀이 및 공부

## 1번

EMR cluster?

## 2번

AWS Batch 란?

> AWS Batch를 사용하면 AWS 클라우드에서 배치 컴퓨팅 워크로드를 실행할 수 있습니다. 배치 컴퓨팅은 개발자, 과학자 및 엔지니어가 대량의 컴퓨팅 리소스에 액세스하는 일반적인 방법입니다. AWS Batch는 기존의 배치 컴퓨팅 소프트웨어와 마찬가지로 필요한 인프라를 구성하고 관리하는 획일적이고 힘든 작업을 제거합니다. 이 서비스는 용량 제약을 제거하고 컴퓨팅 비용을 줄이며 결과를 신속하게 제공하기 위해 제출된 작업에 대한 응답으로 리소스를 효율적으로 프로비저닝할 수 있습니다.

> 완전관리형 서비스인 AWS Batch는 모든 규모의 배치 컴퓨팅 워크로드를 실행할 수 있도록 지원합니다. AWS Batch는 컴퓨팅 리소스를 자동으로 프로비저닝하고 워크로드의 양과 규모에 따라 워크로드 분포를 최적화합니다. AWS Batch를 사용하면 배치 컴퓨팅 소프트웨어를 설치하거나 관리할 필요가 없으므로 결과를 분석하고 문제를 해결하는 데 시간을 집중할 수 있습니다.

![](/assets/2023-03-12-17-44-01.png)

### Components of AWS Batch

- Jobs
- Job Definitions
- Job Queues
- Compute Environment

아래 그림으로 위 컴포넌트들을 대략적으로 이해할 수 있다.

![](/assets/2023-03-12-17-50-21.png)

![](/assets/2023-03-12-17-50-53.png)

콘솔에서 확인해보자.

Step1 - Select orchestration type
![](/assets/2023-03-12-17-53-58.png)

Step2 - Create a compute environment
![](/assets/2023-03-12-17-54-50.png)

Step3 - Create a job queue
![](/assets/2023-03-12-17-55-45.png)

Step4 - Create a job definition
![](/assets/2023-03-12-17-56-51.png)

![](/assets/2023-03-12-17-57-16.png)

Job Definition 을 만들때 어떤 image 를 쓸 것인지 정해준다.

![](/assets/2023-03-12-18-00-03.png)

Job Definition 을 만들때 해당 job의 vCpu, Memory를 정해준다.

Step5 - Create a job
![](/assets/2023-03-12-18-01-24.png)

새로운 job이 생긴 것을 볼 수 있다.

![](/assets/2023-03-12-18-05-12.png)

submitNewJob을 했더니 job이 추가되었다.

방금만든 Job Definition, Job Queue, Compute Environment를 그대로 사용했다.

![](/assets/2023-03-12-18-06-49.png)

DashBoard에서 job들의 상태(실행중, 성공, 실패)를 확인할 수 있다.

![](/assets/2023-03-12-18-11-22.png)

## 마무리

한번 만들어보니까 어떤 서비스인지 알겠다.

> AWS Batch를 사용하면 배치 컴퓨팅 소프트웨어를 설치하거나 관리할 필요가 없다.

이 말이 제일 AWS Batch에 대한 소개를 잘한 것 같다.

# 참고 

- https://stackify.com/aws-batch-guide/
- https://aws.amazon.com/ko/blogs/compute/using-aws-cloudformation-to-create-and-manage-aws-batch-resources/